{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Predicting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samlundberg/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup \n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, LassoCV, Lasso\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring the information from Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#comparing star wars vs. star trek reddits\n",
    "url1 = \"https://reddit.com/r/StarWars.json\"\n",
    "url2= \"https://reddit.com/r/StarTrek.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://reddit.com/r/StarWars.json\n",
      "https://reddit.com/r/StarWars.json?after=t3_9fxcdl\n",
      "https://reddit.com/r/StarWars.json?after=t3_9ftxhu\n",
      "https://reddit.com/r/StarWars.json?after=t3_9fhivw\n",
      "https://reddit.com/r/StarWars.json?after=t3_9fn02b\n",
      "https://reddit.com/r/StarWars.json?after=t3_9frkyp\n",
      "https://reddit.com/r/StarWars.json?after=t3_9fjdeo\n",
      "https://reddit.com/r/StarWars.json?after=t3_9fdgk7\n",
      "https://reddit.com/r/StarWars.json?after=t3_9feak5\n",
      "https://reddit.com/r/StarWars.json?after=t3_9f23f4\n",
      "https://reddit.com/r/StarWars.json?after=t3_9f2j8z\n",
      "https://reddit.com/r/StarWars.json?after=t3_9ew1di\n",
      "https://reddit.com/r/StarWars.json?after=t3_9et7zq\n",
      "https://reddit.com/r/StarWars.json?after=t3_9eomhv\n",
      "https://reddit.com/r/StarWars.json?after=t3_9erkae\n",
      "https://reddit.com/r/StarWars.json?after=t3_9e4ryc\n",
      "https://reddit.com/r/StarWars.json?after=t3_9e4ps6\n",
      "https://reddit.com/r/StarWars.json?after=t3_9e5zk3\n",
      "https://reddit.com/r/StarWars.json?after=t3_9e0v5y\n",
      "https://reddit.com/r/StarWars.json?after=t3_9dt36e\n",
      "https://reddit.com/r/StarWars.json?after=t3_9ducbx\n",
      "https://reddit.com/r/StarWars.json?after=t3_9duzig\n",
      "https://reddit.com/r/StarWars.json?after=t3_9dpvjk\n",
      "https://reddit.com/r/StarWars.json?after=t3_9dksuu\n",
      "https://reddit.com/r/StarWars.json?after=t3_9d9enm\n",
      "https://reddit.com/r/StarTrek.json\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9ft062\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9flo98\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9figfz\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9fbc9u\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9f5f2b\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9f2m1b\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9egqvs\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9eak89\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9ea749\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9duhb6\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9dert6\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9czgmu\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9d2x9g\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9d05dl\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9chis9\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9cfui8\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9c7hsp\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9c15fe\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9buwxd\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9bpuby\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9bg7h8\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9bba83\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9awdlj\n",
      "https://reddit.com/r/StarTrek.json?after=t3_9aeaqj\n"
     ]
    }
   ],
   "source": [
    "#create loop to get 25 new posts at a time,\n",
    "posts=[]\n",
    "#initially set url with default address\n",
    "after=None\n",
    "\n",
    "#pull reddits for each url\n",
    "for j in range(2):\n",
    "    if j==0:\n",
    "        url=url1\n",
    "    else:\n",
    "        url=url2\n",
    "        \n",
    "    after= None\n",
    "    \n",
    "    for i in range(25):\n",
    "\n",
    "        #first time running loop on main url\n",
    "        if after == None:\n",
    "            current_url=url\n",
    "        #change url page to next \n",
    "        else:\n",
    "            current_url=url + '?after=' + after\n",
    "        print(current_url)\n",
    "        res=requests.get(current_url, headers={'User-agent': 'Sam 1.0'})\n",
    "        #check web page status.  if not like 200, stop code\n",
    "        if res.status_code !=200:\n",
    "            print('Status error', res.status_code)\n",
    "            break\n",
    "        current_dict=res.json()\n",
    "        #find all posts and add together\n",
    "        current_posts=[p['data'] for p in current_dict['data']['children']]\n",
    "        posts.extend(current_posts)\n",
    "        #find the next url and set page to pull from next\n",
    "        after=current_dict['data']['after']\n",
    "        #time in between pulling each subreddit\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "    #export to csv file\n",
    "    pd.DataFrame(posts).to_csv('./files/posts.csv', index=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import in csv file with all the posts and create a dataframe\n",
    "df=pd.read_csv('./files/posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy columns for star wars vs star trek to create target varaiable\n",
    "df['target']=df['subreddit'].map({'StarWars':1,\"startrek\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#change column to lowercase\n",
    "df['title'] = df.title.astype(str).str.lower()\n",
    "#remove punctuation\n",
    "df[\"title\"] = df['title'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look for null titles\n",
    "df['title'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only 1 exists ok to change to \"null\"\n",
    "df['title']=df['title'].replace(np.nan,'null')\n",
    "df['title'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1253, 98)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['approved_at_utc', 'approved_by', 'archived', 'author',\n",
       "       'author_cakeday', 'author_flair_background_color',\n",
       "       'author_flair_css_class', 'author_flair_richtext',\n",
       "       'author_flair_template_id', 'author_flair_text',\n",
       "       'author_flair_text_color', 'author_flair_type', 'author_fullname',\n",
       "       'banned_at_utc', 'banned_by', 'can_gild', 'can_mod_post', 'category',\n",
       "       'clicked', 'content_categories', 'contest_mode', 'created',\n",
       "       'created_utc', 'crosspost_parent', 'crosspost_parent_list',\n",
       "       'distinguished', 'domain', 'downs', 'edited', 'gilded', 'hidden',\n",
       "       'hide_score', 'id', 'is_crosspostable', 'is_meta',\n",
       "       'is_original_content', 'is_reddit_media_domain', 'is_self', 'is_video',\n",
       "       'likes', 'link_flair_background_color', 'link_flair_css_class',\n",
       "       'link_flair_richtext', 'link_flair_template_id', 'link_flair_text',\n",
       "       'link_flair_text_color', 'link_flair_type', 'locked', 'media',\n",
       "       'media_embed', 'media_metadata', 'media_only', 'mod_note',\n",
       "       'mod_reason_by', 'mod_reason_title', 'mod_reports', 'name', 'no_follow',\n",
       "       'num_comments', 'num_crossposts', 'num_reports', 'over_18',\n",
       "       'parent_whitelist_status', 'permalink', 'pinned', 'post_hint',\n",
       "       'preview', 'pwls', 'quarantine', 'removal_reason', 'report_reasons',\n",
       "       'saved', 'score', 'secure_media', 'secure_media_embed', 'selftext',\n",
       "       'selftext_html', 'send_replies', 'spoiler', 'stickied', 'subreddit',\n",
       "       'subreddit_id', 'subreddit_name_prefixed', 'subreddit_subscribers',\n",
       "       'subreddit_type', 'suggested_sort', 'thumbnail', 'thumbnail_height',\n",
       "       'thumbnail_width', 'title', 'ups', 'url', 'user_reports', 'view_count',\n",
       "       'visited', 'whitelist_status', 'wls', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#find url address for each row\n",
    "dfurl=df['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       t5_2qi4s\n",
       "1       t5_2qi4s\n",
       "2       t5_2qi4s\n",
       "3       t5_2qi4s\n",
       "4       t5_2qi4s\n",
       "5       t5_2qi4s\n",
       "6       t5_2qi4s\n",
       "7       t5_2qi4s\n",
       "8       t5_2qi4s\n",
       "9       t5_2qi4s\n",
       "10      t5_2qi4s\n",
       "11      t5_2qi4s\n",
       "12      t5_2qi4s\n",
       "13      t5_2qi4s\n",
       "14      t5_2qi4s\n",
       "15      t5_2qi4s\n",
       "16      t5_2qi4s\n",
       "17      t5_2qi4s\n",
       "18      t5_2qi4s\n",
       "19      t5_2qi4s\n",
       "20      t5_2qi4s\n",
       "21      t5_2qi4s\n",
       "22      t5_2qi4s\n",
       "23      t5_2qi4s\n",
       "24      t5_2qi4s\n",
       "25      t5_2qi4s\n",
       "26      t5_2qi4s\n",
       "27      t5_2qi4s\n",
       "28      t5_2qi4s\n",
       "29      t5_2qi4s\n",
       "          ...   \n",
       "1223    t5_2qixm\n",
       "1224    t5_2qixm\n",
       "1225    t5_2qixm\n",
       "1226    t5_2qixm\n",
       "1227    t5_2qixm\n",
       "1228    t5_2qixm\n",
       "1229    t5_2qixm\n",
       "1230    t5_2qixm\n",
       "1231    t5_2qixm\n",
       "1232    t5_2qixm\n",
       "1233    t5_2qixm\n",
       "1234    t5_2qixm\n",
       "1235    t5_2qixm\n",
       "1236    t5_2qixm\n",
       "1237    t5_2qixm\n",
       "1238    t5_2qixm\n",
       "1239    t5_2qixm\n",
       "1240    t5_2qixm\n",
       "1241    t5_2qixm\n",
       "1242    t5_2qixm\n",
       "1243    t5_2qixm\n",
       "1244    t5_2qixm\n",
       "1245    t5_2qixm\n",
       "1246    t5_2qixm\n",
       "1247    t5_2qixm\n",
       "1248    t5_2qixm\n",
       "1249    t5_2qixm\n",
       "1250    t5_2qixm\n",
       "1251    t5_2qixm\n",
       "1252    t5_2qixm\n",
       "Name: subreddit_id, Length: 1253, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find subredit id for each row\n",
    "subredditid=df['subreddit_id']\n",
    "df['subreddit_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change created_utc dtype from float64 to int to properly analyze number format\n",
    "df['created_utc'] = pd.DataFrame(df['created_utc'], dtype='int')\n",
    "df['created_utc'].dtypes\n",
    "time_stamp=df['created_utc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1531300998,\n",
       " 1536670618,\n",
       " 1537016991,\n",
       " 1537019636,\n",
       " 1537010479,\n",
       " 1536971978,\n",
       " 1536961286,\n",
       " 1537010250,\n",
       " 1536991295,\n",
       " 1537021382]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create list from time stamp column\n",
    "time_stamp_list=time_stamp.tolist()\n",
    "time_stamp_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1253"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_stamp_count=time_stamp.count()\n",
    "time_stamp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert utc timestamp to readable times\n",
    "\n",
    "def pretty_date(time=False):\n",
    "\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    if type(time) is int:\n",
    "        diff = now - datetime.fromtimestamp(time)\n",
    "    elif isinstance(time,datetime):\n",
    "        diff = round(now - time,2)\n",
    "    elif not time:\n",
    "        diff = round(now - now,2)\n",
    "    second_diff = diff.seconds\n",
    "    day_diff = diff.days\n",
    "\n",
    "    if day_diff < 0:\n",
    "        return ''\n",
    "\n",
    "    if day_diff == 0:\n",
    "        if second_diff < 10:\n",
    "            return \"just now\"\n",
    "        if second_diff < 60:\n",
    "            return str(second_diff) + \" seconds ago\"\n",
    "        if second_diff < 120:\n",
    "            return \"a minute ago\"\n",
    "        if second_diff < 3600:\n",
    "            return str(second_diff / 60) + \" minutes ago\"\n",
    "        if second_diff < 7200:\n",
    "            return \"an hour ago\"\n",
    "        if second_diff < 86400:\n",
    "            return str(second_diff / 3600) + \" hours ago\"\n",
    "    if day_diff == 1:\n",
    "        return \"Yesterday\"\n",
    "    if day_diff < 7:\n",
    "        return str(day_diff) + \" days ago\"\n",
    "    if day_diff < 31:\n",
    "        return str(day_diff / 7) + \" weeks ago\"\n",
    "    if day_diff < 365:\n",
    "        return str(day_diff / 30) + \" months ago\"\n",
    "    return str(day_diff / 365) + \" years ago\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5 months ago'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretty_date(1531300998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create loop to iterate through each utc timestamps and convert to 'normal' values\n",
    "pretty_date_new=[]\n",
    "\n",
    "for j in time_stamp_list:\n",
    "    pretty_date_updated=pretty_date(j)\n",
    "    pretty_date_new.append(pretty_date_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to df\n",
    "pretty_date_new_df=pd.DataFrame(pretty_date_new)\n",
    "pdn = pd.Series(pretty_date_new)\n",
    "df['time_passed']=pdn.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                          on opinions\n",
       "1                         solo home release megathread\n",
       "2    i was scoutmaster for a bsa national youth lea...\n",
       "3    say what you will about solo im just so glad w...\n",
       "4                          the real star wars universe\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find titles of each post\n",
    "titles=df['title']\n",
    "df['title'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StarWars    627\n",
       "startrek    626\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find subreddit names\n",
    "subreddit=df['subreddit'].value_counts()\n",
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4 months ago                   104\n",
       "1.4333333333333333 months ago    103\n",
       "1.3666666666666667 months ago    101\n",
       "1.3333333333333333 months ago     98\n",
       "1.5666666666666667 months ago     98\n",
       "1.5333333333333334 months ago     95\n",
       "1.4666666666666666 months ago     88\n",
       "1.6 months ago                    81\n",
       "1.3 months ago                    69\n",
       "1.6333333333333333 months ago     62\n",
       "1.5 months ago                    52\n",
       "1.9 months ago                    38\n",
       "1.7 months ago                    37\n",
       "1.7666666666666666 months ago     35\n",
       "1.8 months ago                    35\n",
       "1.6666666666666667 months ago     33\n",
       "1.7333333333333334 months ago     29\n",
       "1.8333333333333333 months ago     29\n",
       "1.9333333333333333 months ago     26\n",
       "1.8666666666666667 months ago     22\n",
       "1.9666666666666666 months ago     16\n",
       "2.0 months ago                     1\n",
       "3.5 months ago                     1\n",
       "Name: time_passed, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of time thread has been up\n",
    "time_passed=df['time_passed'].value_counts()\n",
    "df['time_passed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    740\n",
       "1    223\n",
       "2    160\n",
       "3    179\n",
       "4     45\n",
       "Name: num_comments, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find number of comments per thread\n",
    "num_comments=df['num_comments']\n",
    "df['num_comments'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as a cleaned csv file\n",
    "pd.DataFrame(df).to_csv('./files/cleanposts.csv', index=False)   "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
