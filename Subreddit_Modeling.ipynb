{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "# Predicting Reddit comments using Random Forests and Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup \n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, LassoCV, Lasso\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [],
   "source": [
    "#import in csv file with all the cleaned posts and create a dataframe\n",
    "df=pd.read_csv('./files/cleanposts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for missing titles and replace with null\n",
    "#only 1 exists ok to change to \"null\"\n",
    "df['title']=df['title'].replace(np.nan,'null')\n",
    "df['title'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defind variables\n",
    "X=df['title']  \n",
    "y=df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y,stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#double check for missing values\n",
    "X_train=X_train.replace(np.nan,'null')\n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer()   \n",
    "model=MultinomialNB()\n",
    "\n",
    "pipe=Pipeline([('cv',cv),('model',model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Train Score :  0.8615548455804047\n",
      "GS TestScore   :  0.8757961783439491\n",
      "Optimal Param  :  {}\n"
     ]
    }
   ],
   "source": [
    "#initial gridsearch untuned with countvectorizer\n",
    "params={}\n",
    "gs=GridSearchCV(pipe, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_score_\n",
    "gs_test=gs.score(X_test,y_test)\n",
    "print('GS Train Score : ',gs.best_score_)\n",
    "print('GS TestScore   : ',gs_test)\n",
    "print('Optimal Param  : ',gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Train Score :  0.8722044728434505\n",
      "GS TestScore   :  0.8630573248407644\n",
      "Optimal Param  :  {'cv__max_df': 0.5, 'cv__max_features': 3000, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "#tuning gridsearch by stop words and max features, ngrams\n",
    "params={'cv__stop_words':[None,'english'],\n",
    "        'cv__max_features':[1900,3000],\n",
    "        'cv__ngram_range':[(1,1),(1,2)],\n",
    "        'cv__max_df':[0.5, 0.7, 1.0]}\n",
    "gs=GridSearchCV(pipe, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_score_\n",
    "gs_test=gs.score(X_test,y_test)\n",
    "print('GS Train Score : ',gs.best_score_)\n",
    "print('GS TestScore   : ',gs_test)\n",
    "print('Optimal Param  : ',gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Multinomial model, using Count Vectorizer, there were no changes when applying parameters, such as stop words or max features as part of the count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using BernoulliNB\n",
    "cv=CountVectorizer(binary=True)   #binary=True\n",
    "model=BernoulliNB()\n",
    "\n",
    "pipeb=Pipeline([('cv',cv),('model',model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Train Score :  0.8668796592119276\n",
      "GS Test Score  :  0.8503184713375797\n",
      "Optimal Param  :  {}\n"
     ]
    }
   ],
   "source": [
    "#initial gridsearch untuned\n",
    "params={}\n",
    "gs=GridSearchCV(pipeb, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_score_\n",
    "gs_test=gs.score(X_test,y_test)\n",
    "print('GS Train Score : ',gs.best_score_)\n",
    "print('GS Test Score  : ',gs_test)\n",
    "print('Optimal Param  : ',gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Train Score :  0.8668796592119276\n",
      "GS Test Score :  0.8789808917197452\n",
      "Optimal Param  :  {'cv__max_features': 2500, 'cv__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "#tuning gridsearch by stop words and max features\n",
    "params={'cv__stop_words':[None,'english'],\n",
    "        'cv__max_features':[1500,2000,2500]}\n",
    "gs=GridSearchCV(pipeb, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_score_\n",
    "gs_test=gs.score(X_test,y_test)\n",
    "print('GS Train Score : ',gs.best_score_)\n",
    "print('GS Test Score : ',gs_test)\n",
    "print('Optimal Param  : ',gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Train Score :  0.8668796592119276\n",
      "GS Test Score  :  0.8789808917197452\n",
      "Optimal Param  :  {'cv__max_df': 0.5, 'cv__max_features': 2500, 'cv__ngram_range': (1, 1), 'cv__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "#tuning gridsearch by stop words and max features, ngrams\n",
    "params={'cv__stop_words':[None,'english'],\n",
    "        'cv__max_features':[1500,2000,2500],\n",
    "        'cv__ngram_range':[(1,1),(1,2)],\n",
    "        'cv__max_df':[0.5, 0.7, 1.0]}\n",
    "gs=GridSearchCV(pipeb, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_score_\n",
    "gs_test=gs.score(X_test,y_test)\n",
    "print('GS Train Score : ',gs.best_score_)\n",
    "print('GS Test Score  : ',gs_test)\n",
    "print('Optimal Param  : ',gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Count Vectorizer and BernoulliNB model, with similar parameters as MultinomialNB, increased the accuracy by 1%.  Still needs some work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF-IDF and Hash Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and fit model\n",
    "#remove english stop words\n",
    "t_vec = TfidfVectorizer(stop_words = 'english')\n",
    "h_vec = HashingVectorizer(stop_words = 'english')\n",
    "\n",
    "#fit & transform to tfidf model\n",
    "t_vec_train = t_vec.fit_transform(X_train, y_train)\n",
    "t_vec_test = t_vec.transform(X_test)\n",
    "\n",
    "#fit & transform to hash model\n",
    "h_vec_train = h_vec.fit_transform(X_train, y_train)\n",
    "h_vec_test  = h_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2702"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# of features for the tfidf model\n",
    "len(t_vec_train.toarray().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048576"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# of features for the hash model\n",
    "len(h_vec_train.toarray().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Log Reg Train Score :  0.9914802981895634\n",
      "Ridge Log Reg Test Score  :  0.8630573248407644\n"
     ]
    }
   ],
   "source": [
    "# model and performance of the TFIDF on lr with ridge penalty\n",
    "lr = LogisticRegression(penalty = 'l2')\n",
    "\n",
    "lr.fit(t_vec_train, y_train)\n",
    "print('Ridge Log Reg Train Score : ', lr.score(t_vec_train, y_train))\n",
    "print('Ridge Log Reg Test Score  : ',lr.score(t_vec_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Log Reg Train Score :  0.8104366347177849\n",
      "Lasso Log Reg Test Score  :  0.7961783439490446\n"
     ]
    }
   ],
   "source": [
    "# model and performance of the TFIDF on lr with lasso penalty\n",
    "lr = LogisticRegression(penalty = 'l1')\n",
    "\n",
    "lr.fit(t_vec_train, y_train)\n",
    "print('Lasso Log Reg Train Score : ', lr.score(t_vec_train, y_train))\n",
    "print('Lasso Log Reg Test Score  : ',lr.score(t_vec_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Hash Train Score :  0.9829605963791267\n",
      "Ridge Hash Test Score  :  0.8407643312101911\n"
     ]
    }
   ],
   "source": [
    "# model and performance of the hash on lr with ridge penalty\n",
    "lr = LogisticRegression(penalty = 'l2')\n",
    "\n",
    "lr.fit(h_vec_train, y_train)\n",
    "print('Ridge Hash Train Score : ', lr.score(h_vec_train, y_train))\n",
    "print('Ridge Hash Test Score  : ',lr.score(h_vec_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Hash Train Score :  0.8125665601703941\n",
      "Lasso Hash Test Score  :  0.8057324840764332\n"
     ]
    }
   ],
   "source": [
    "# model and performance of the hash on lr with lasso penalty\n",
    "lr = LogisticRegression(penalty = 'l1')\n",
    "\n",
    "lr.fit(h_vec_train, y_train)\n",
    "print('Lasso Hash Train Score : ', lr.score(h_vec_train, y_train))\n",
    "print('Lasso Hash Test Score  : ',lr.score(h_vec_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF and Hash, they generally did worse as far as scoring, with the exception of using Ridge.  This jumped up quite high, but overfitting between the train and test data sets.  So far this is the best model.  To further check out this model, time to run it through Grid Search to further optimize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [(\"tf_idf\", TfidfVectorizer()), \n",
    "              (\"ridge\", linear_model.RidgeClassifier())]\n",
    "model = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Train TFIDF Ridge Score  :  0.8647497337593184\n",
      "GS Test TFIDF Ridge Score   :  0.8726114649681529\n",
      "Optimal Param TFIDF Ridge   :  {'ridge__alpha': 1, 'tf_idf__max_df': 0.2, 'tf_idf__min_df': 1, 'tf_idf__ngram_range': (1, 7), 'tf_idf__stop_words': 'english', 'tf_idf__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "#tfidf with ridge stop words removed and optomized\n",
    "params = {\"ridge__alpha\":[1, 3, 5],               #regularization param\n",
    "          \"tf_idf__min_df\": [1, 3],               #min count of words allowed\n",
    "          \"tf_idf__ngram_range\": [(1,7), (1,8)],  #1-grams or 2-grams\n",
    "          \"tf_idf__stop_words\": [None, \"english\"],#use stopwords or not\n",
    "          \"tf_idf__use_idf\":[True, False],        #whether to scale columns or just leave normalized bag of words.\n",
    "          \"tf_idf__max_df\": [0.2, 0.3, 0.4]}      #max count of words allowed\n",
    "gs=GridSearchCV(estimator=model,param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_score_\n",
    "gs_test=gs.score(X_test,y_test)\n",
    "print('GS Train TFIDF Ridge Score  : ',gs.best_score_)\n",
    "print('GS Test TFIDF Ridge Score   : ',gs_test)\n",
    "print('Optimal Param TFIDF Ridge   : ',gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Grid Search, TF_IDF with Ridge penalty, the model came back to earth and more in line with the Count Vectorizations.  The model became underfit instead of the inital overfit, but the score dropped a hair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run through cv model, change ngram range from (1,1) to (1,2)\n",
    "cvec=CountVectorizer(ngram_range=(1,2),stop_words='english')\n",
    "\n",
    "# Fit & transform the vectorizer on our corpus\n",
    "data_vector = cvec.fit_transform(X_train)\n",
    "#data_vector_test = cvec.transform(y_test)\n",
    "\n",
    "\n",
    "#create df to view data\n",
    "df_cvec_1  = pd.DataFrame(data_vector.toarray(),\n",
    "                     columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Train Score :  0.8487752928647497\n",
      "Optimal Param  :  {'criterion': 'entropy', 'max_depth': 11, 'max_features': 'log2', 'n_estimators': 900}\n"
     ]
    }
   ],
   "source": [
    "#create random forest model\n",
    "rfc=RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [900, 1000],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [10,11,12],\n",
    "    'criterion' :['gini', 'entropy']}\n",
    "\n",
    "gs = GridSearchCV(estimator=rfc, param_grid=param_grid)\n",
    "gs.fit(data_vector, y_train)\n",
    "gs.best_score_\n",
    "print('GS Train Score : ',gs.best_score_)\n",
    "print('Optimal Param  : ',gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the same information through a Random Forest model decreases the score a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "### We want to predict a binary variable - whether the number of comments was low or high. Computing the median number of comments and create a new binary variable that is true when the number of comments is high (above the median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change num_comments field from int to float\n",
    "df['num_comments']=df['num_comments'].astype(int).astype(float)\n",
    "df['num_comments'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.25778132482043"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find mean of number of comments\n",
    "mean_comments=np.mean(df['num_comments'])\n",
    "np.mean(df['num_comments'])\n",
    "#mean number of comments is 33.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_quartile_comments=np.percentile(df.num_comments, 50) \n",
    "bottom_quartile_comments=np.percentile(df.num_comments, 25) \n",
    "upper_quartile_comments=np.percentile(df.num_comments, 75) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1253.000000\n",
       "mean       32.257781\n",
       "std        71.814044\n",
       "min         0.000000\n",
       "25%         5.000000\n",
       "50%        12.000000\n",
       "75%        30.000000\n",
       "max      1015.000000\n",
       "Name: num_comments, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['num_comments'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign 1 to comments over 33.06 or 0 to posts under 33.06\n",
    "df['comment_check_mean'] = [0 if num_comment_list < mean_comments \n",
    "                       else 1 for num_comment_list in df['num_comments']]\n",
    "\n",
    "#25th percentile\n",
    "df['comment_check_25'] = [0 if num_comment_list < bottom_quartile_comments  \n",
    "                       else 1 for num_comment_list in df['num_comments']]\n",
    "\n",
    "#75th percentile\n",
    "df['comment_check_75'] = [0 if num_comment_list < upper_quartile_comments  \n",
    "                       else 1 for num_comment_list in df['num_comments']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    965\n",
       "1    288\n",
       "Name: comment_check_mean, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of high (283) vs low (969) comments compared to mean of 33.06\n",
    "df['comment_check_mean'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    945\n",
       "0    308\n",
       "Name: comment_check_25, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of comments in 25% quartile of num_comments\n",
    "df['comment_check_25'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    936\n",
       "1    317\n",
       "Name: comment_check_75, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of comments in 75% quartile of num_comments\n",
    "df['comment_check_75'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.500399\n",
       "0    0.499601\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline accuracy of model using subreddit\n",
    "df['target'].value_counts(normalize=True)\n",
    "#this means 50% of posts are related to teh subreddit starwars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.770152\n",
       "1    0.229848\n",
       "Name: comment_check_mean, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline accuracy of model using mean of comments\n",
    "df['comment_check_mean'].value_counts(normalize=True)\n",
    "#this means 77.4% of posts have less than the mean of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.75419\n",
       "0    0.24581\n",
       "Name: comment_check_25, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline accuracy of model using 25th perncentile of comments\n",
    "df['comment_check_25'].value_counts(normalize=True)\n",
    "#this means 75.0% of posts have less than t?he 25th percentile of number of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.747007\n",
       "1    0.252993\n",
       "Name: comment_check_75, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline accuracy of model using 75th perncentile of comments\n",
    "df['comment_check_75'].value_counts(normalize=True)\n",
    "#this means 74.9% of posts have less than the 75th percentile of number of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "### Create a Random Forest model to predict High/Low number of comments usinging the subreddit as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set varaiables, new cross train split\n",
    "\n",
    "X=df['title']\n",
    "y=df['comment_check_mean']\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>...</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "      <th>target</th>\n",
       "      <th>time_passed</th>\n",
       "      <th>comment_check_mean</th>\n",
       "      <th>comment_check_25</th>\n",
       "      <th>comment_check_75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Yunners</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21 thisFlairHasText</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jedi Knight</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5 months ago</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Yunners</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21 thisFlairHasText</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jedi Knight</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4333333333333333 months ago</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>getridofwires</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3 months ago</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>noslowsongs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3 months ago</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>aditseth03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3 months ago</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   approved_at_utc  approved_by  archived         author author_cakeday  \\\n",
       "0              NaN          NaN     False        Yunners            NaN   \n",
       "1              NaN          NaN     False        Yunners            NaN   \n",
       "2              NaN          NaN     False  getridofwires           True   \n",
       "3              NaN          NaN     False    noslowsongs            NaN   \n",
       "4              NaN          NaN     False     aditseth03            NaN   \n",
       "\n",
       "   author_flair_background_color author_flair_css_class author_flair_richtext  \\\n",
       "0                            NaN    21 thisFlairHasText                    []   \n",
       "1                            NaN    21 thisFlairHasText                    []   \n",
       "2                            NaN                     42                    []   \n",
       "3                            NaN                    NaN                    []   \n",
       "4                            NaN                      9                    []   \n",
       "\n",
       "  author_flair_template_id author_flair_text       ...        user_reports  \\\n",
       "0                      NaN       Jedi Knight       ...                  []   \n",
       "1                      NaN       Jedi Knight       ...                  []   \n",
       "2                      NaN               NaN       ...                  []   \n",
       "3                      NaN               NaN       ...                  []   \n",
       "4                      NaN               NaN       ...                  []   \n",
       "\n",
       "  view_count visited  whitelist_status  wls  target  \\\n",
       "0        NaN   False           all_ads    6       1   \n",
       "1        NaN   False           all_ads    6       1   \n",
       "2        NaN   False           all_ads    6       1   \n",
       "3        NaN   False           all_ads    6       1   \n",
       "4        NaN   False           all_ads    6       1   \n",
       "\n",
       "                     time_passed comment_check_mean  comment_check_25  \\\n",
       "0                 3.5 months ago                  1                 1   \n",
       "1  1.4333333333333333 months ago                  1                 1   \n",
       "2                 1.3 months ago                  1                 1   \n",
       "3                 1.3 months ago                  1                 1   \n",
       "4                 1.3 months ago                  1                 1   \n",
       "\n",
       "  comment_check_75  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countvectorizer on subreddit and title to determine if # of comments above/below mean\n",
    "\n",
    "cvec=CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit & transform the vectorizer on our corpus\n",
    "data_vector = cvec.fit_transform(X_train)\n",
    "#data_vector_test = cvec.transform(y_test)\n",
    "\n",
    "#create df to view data\n",
    "df_cvec  = pd.DataFrame(data_vector.toarray(),\n",
    "                    columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100th</th>\n",
       "      <th>11001001</th>\n",
       "      <th>12</th>\n",
       "      <th>129</th>\n",
       "      <th>13</th>\n",
       "      <th>13m</th>\n",
       "      <th>13th</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>...</th>\n",
       "      <th>younglings</th>\n",
       "      <th>youre</th>\n",
       "      <th>youth</th>\n",
       "      <th>youve</th>\n",
       "      <th>yuuzhan</th>\n",
       "      <th>yvanquinet</th>\n",
       "      <th>ywings</th>\n",
       "      <th>zahn</th>\n",
       "      <th>zefram</th>\n",
       "      <th>ziyal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2782 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  100th  11001001  12  129  13  13m  13th  14  15  ...    younglings  \\\n",
       "0   0      0         0   0    0   0    0     0   0   0  ...             0   \n",
       "1   0      0         0   0    0   0    0     0   0   0  ...             0   \n",
       "2   0      0         0   0    0   0    0     0   0   0  ...             0   \n",
       "3   0      0         0   0    0   0    0     0   0   0  ...             0   \n",
       "4   1      0         0   0    0   0    0     0   0   0  ...             0   \n",
       "\n",
       "   youre  youth  youve  yuuzhan  yvanquinet  ywings  zahn  zefram  ziyal  \n",
       "0      0      1      0        0           0       0     0       0      0  \n",
       "1      0      0      0        0           0       0     0       0      0  \n",
       "2      0      0      0        0           0       0     0       0      0  \n",
       "3      0      0      0        0           0       0     0       0      0  \n",
       "4      0      0      0        0           0       0     0       0      0  \n",
       "\n",
       "[5 rows x 2782 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cvec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Train Score :  0.7699680511182109\n",
      "Optimal Param  :  {'criterion': 'gini', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "#create random forest model\n",
    "rfc=RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']}\n",
    "\n",
    "gs = GridSearchCV(estimator=rfc, param_grid=param_grid)\n",
    "gs.fit(data_vector, y_train)\n",
    "gs.best_score_\n",
    "print('GS Train Score : ',gs.best_score_)\n",
    "print('Optimal Param  : ',gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Grid Search and Random Forest, the model performed worse than any of the others, by about 7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining other features to see if they are helpful in modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of top words in titles COMBINED\n",
    "word_count_df = pd.DataFrame( columns = ['Word','Count'])\n",
    "\n",
    "for col in df_cvec.columns:\n",
    "    word_count_df.loc[len(word_count_df)] = [col, df_cvec[col].sum()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>star</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2523</th>\n",
       "      <td>trek</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>wars</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>new</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>like</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>just</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>tng</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>series</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>ds9</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>solo</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>time</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2459</th>\n",
       "      <td>think</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>jedi</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>episode</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>picard</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>enterprise</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>discovery</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>does</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>favorite</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>thought</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word Count\n",
       "2307        star   230\n",
       "2523        trek   156\n",
       "2673        wars    90\n",
       "1607         new    48\n",
       "1374        like    45\n",
       "1269        just    41\n",
       "2491         tng    41\n",
       "2146      series    31\n",
       "721          ds9    29\n",
       "2252        solo    29\n",
       "2481        time    28\n",
       "2459       think    28\n",
       "1247        jedi    25\n",
       "795      episode    25\n",
       "1750      picard    25\n",
       "784   enterprise    24\n",
       "670    discovery    23\n",
       "691         does    21\n",
       "873     favorite    20\n",
       "2462     thought    20"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_df.sort_values('Count', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy columns episode and new to df\n",
    "df['episode']=df_cvec['episode'].copy()\n",
    "df['new']=df_cvec['new'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fill missing values on posts without episode with a 0, same with new\n",
    "df['episode']=df['episode'].replace(np.nan,'0').astype(int)\n",
    "df['new']=df['new'].replace(np.nan,'0').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1229\n",
       "1      23\n",
       "2       1\n",
       "Name: episode, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['episode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix outlier in episode column of 2.  make it a 0\n",
    "df['episode']=df['episode'].replace(2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set new variables and new train test split\n",
    "X=df['subreddit']\n",
    "y=df['episode']\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run through cv model, change ngram range from (1,1) to (1,2)\n",
    "cvec=CountVectorizer(ngram_range=(1,2),stop_words='english')\n",
    "\n",
    "# Fit & transform the vectorizer on our corpus\n",
    "data_vector = cvec.fit_transform(X_train)\n",
    "#data_vector_test = cvec.transform(y_test)\n",
    "\n",
    "#create df to view data\n",
    "df_cvec_1  = pd.DataFrame(data_vector.toarray(),\n",
    "                     columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Train Score :  0.9818956336528222\n",
      "Optimal Param  :  {'criterion': 'gini', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "#put cv model into random forest\n",
    "rf=RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']}\n",
    "\n",
    "gs = GridSearchCV(estimator=rf, param_grid=param_grid)\n",
    "gs.fit(data_vector, y_train)\n",
    "gs.best_score_\n",
    "print('GS Train Score : ',gs.best_score_)\n",
    "print('Optimal Param  : ',gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the word Episode to see if it fits into the appropriate subreddit, has quite good success to match to the Star Wars Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#set new variables and new train test split\n",
    "X=df['title']\n",
    "y=df['episode']\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run through cv model, change ngram range from (1,1) to (1,2)\n",
    "cvec=CountVectorizer(ngram_range=(1,2),stop_words='english')\n",
    "\n",
    "# Fit & transform the vectorizer on our corpus\n",
    "data_vector = cvec.fit_transform(X_train)\n",
    "#data_vector_test = cvec.transform(y_test)\n",
    "\n",
    "#create df to view data\n",
    "df_cvec_1  = pd.DataFrame(data_vector.toarray(),\n",
    "                     columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Train Score :  0.9818956336528222\n",
      "Optimal Param  :  {'criterion': 'gini', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "#put cv model into random forest\n",
    "rf=RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']}\n",
    "\n",
    "gs = GridSearchCV(estimator=rf, param_grid=param_grid)\n",
    "gs.fit(data_vector, y_train)\n",
    "gs.best_score_\n",
    "print('GS Train Score : ',gs.best_score_)\n",
    "print('Optimal Param  : ',gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the word 'episode' in the title of the post, shows a very high score, so there is a strong correlation between the two.  Now to try episode to predict reddit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set new variables and new train test split\n",
    "X=df['subreddit']\n",
    "y=df['episode']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run through cv model, change ngram range from (1,1) to (1,2)\n",
    "cvec=CountVectorizer(ngram_range=(1,2),stop_words='english')\n",
    "\n",
    "# Fit & transform the vectorizer on our corpus\n",
    "data_vector = cvec.fit_transform(X_train)\n",
    "#data_vector_test = cvec.transform(y_test)\n",
    "\n",
    "#create df to view data\n",
    "df_cvec_1  = pd.DataFrame(data_vector.toarray(),\n",
    "                     columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Train Score :  0.9818956336528222\n",
      "Optimal Param  :  {'criterion': 'gini', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "#put cv model into random forest\n",
    "rf=RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']}\n",
    "\n",
    "gs = GridSearchCV(estimator=rf, param_grid=param_grid)\n",
    "gs.fit(data_vector, y_train)\n",
    "gs.best_score_\n",
    "print('GS Train Score : ',gs.best_score_)\n",
    "print('Optimal Param  : ',gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The word Episode appears to be able to correctly predict the Star Wards Reddit 97% of the time, which is surprising given subject matter expertise.  The Star Wars movies are each an episode, but there are also countless Star Trek TV episodes.  One would think that episode would be more closely tied to Star Trek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['new'] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set new variables and new train test split\n",
    "X=df['title']\n",
    "y=df['new']\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run through cv model, change ngram range from (1,1) to (1,2)\n",
    "cvec=CountVectorizer(ngram_range=(1,2),stop_words='english')\n",
    "\n",
    "# Fit & transform the vectorizer on our corpus\n",
    "data_vector = cvec.fit_transform(X_train)\n",
    "#data_vector_test = cvec.transform(y_test)\n",
    "\n",
    "#create df to view data\n",
    "df_cvec_1  = pd.DataFrame(data_vector.toarray(),\n",
    "                     columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Train Score :  0.9627263045793397\n",
      "Optimal Param  :  {'criterion': 'gini', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "#put cv model into random forest\n",
    "rf=RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']}\n",
    "\n",
    "gs = GridSearchCV(estimator=rf, param_grid=param_grid)\n",
    "gs.fit(data_vector, y_train)\n",
    "gs.best_score_\n",
    "print('GS Train Score : ',gs.best_score_)\n",
    "print('Optimal Param  : ',gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the word \"New\" also has strong results to predict Star Wars based posts at 96%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set new variables and new train test split\n",
    "X=df['subreddit']\n",
    "y=df['new']\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run through cv model, change ngram range from (1,1) to (1,2)\n",
    "cvec=CountVectorizer(ngram_range=(1,2),stop_words='english')\n",
    "\n",
    "# Fit & transform the vectorizer on our corpus\n",
    "data_vector = cvec.fit_transform(X_train)\n",
    "#data_vector_test = cvec.transform(y_test)\n",
    "\n",
    "#create df to view data\n",
    "df_cvec_1  = pd.DataFrame(data_vector.toarray(),\n",
    "                     columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Train Score :  0.9627263045793397\n",
      "Optimal Param  :  {'criterion': 'gini', 'max_depth': 1, 'max_features': 'auto', 'n_estimators': 1}\n"
     ]
    }
   ],
   "source": [
    "#put cv model into random forest\n",
    "rf=RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [1, 25],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [1,2,3],\n",
    "    'criterion' :['gini', 'entropy']}\n",
    "\n",
    "gs = GridSearchCV(estimator=rf, param_grid=param_grid)\n",
    "gs.fit(data_vector, y_train)\n",
    "gs.best_score_\n",
    "print('GS Train Score : ',gs.best_score_)\n",
    "print('Optimal Param  : ',gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the word \"New\" was able to predict the Star Wars subreddit 96.4% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "## Use cross-validation to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross val 0.9627294570290236\n"
     ]
    }
   ],
   "source": [
    "#cross val score of rf and episode\n",
    "\n",
    "print('cross val',cross_val_score(rf, data_vector, y_train).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat the model-building process with a non-tree-based method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = list(ENGLISH_STOP_WORDS) + ['star','wars','trek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['title']\n",
    "y=df['episode']\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS Train TFIDF Ridge Score  :  0.9818956336528222\n",
      "Optimal Param TFIDF Ridge   :  {'ridge__alpha': 1, 'tf_idf__max_df': 0.2, 'tf_idf__min_df': 1, 'tf_idf__ngram_range': (1, 2), 'tf_idf__stop_words': None, 'tf_idf__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "estimators = [(\"tf_idf\", TfidfVectorizer()), \n",
    "              (\"ridge\", linear_model.RidgeClassifier())]\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "#tfidf with ridge stop words removed and optomized\n",
    "params = {\"ridge__alpha\":[1, 3, 5],               #regularization param\n",
    "          \"tf_idf__min_df\": [1, 3],               #min count of words allowed\n",
    "          \"tf_idf__ngram_range\": [(1,2)],  #1-grams or 2-grams\n",
    "          \"tf_idf__stop_words\": [None, \"english\"],#use stopwords or not\n",
    "          \"tf_idf__use_idf\":[True, False],        #whether to scale columns or just leave normalized bag of words.\n",
    "          \"tf_idf__max_df\": [0.2, 0.3, 0.4]}      #max count of words allowed\n",
    "gs=GridSearchCV(estimator=model,param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_score_\n",
    "print('GS Train TFIDF Ridge Score  : ',gs.best_score_)\n",
    "print('Optimal Param TFIDF Ridge   : ',gs.best_params_)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
